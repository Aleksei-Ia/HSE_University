{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtBEI1lh62ep4viP+h+F15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleksei-Ia/HSE_University/blob/main/Math_homework_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 1 (2 балла). Бывший технический директор Open AI Мира выписала на листочке функцию $f(x)=x\\cdot\\ln(x)$ и просит вас найти её минимум. Помогите ей.\n",
        "\n",
        "Найдём производную:\n",
        "\n",
        "$f'(x)=\\frac{d}{dx}(x\\cdot\\ln(x))=\\ln(x)+x\\cdot\\frac{1}{x}=\\ln(x)+1$\n",
        "\n",
        "Приравняем производную к нулю:\n",
        "\n",
        "$\\ln(x)+1=0$\n",
        "\n",
        "$\\ln(x) = -1$$ $$x = e^{-1} = \\frac{1}{e}$\n",
        "\n",
        "Определим, является ли эта критическая точка минимумом:\n",
        "\n",
        "$f''(x)=\\frac{d}{dx}(\\ln(x)+1)=\\frac{1}{x}$\n",
        "\n",
        "$x = \\frac{1}{e}\\implies f''\\left(\\frac{1}{e}\\right)=e>0$\n",
        "\n",
        "Так как вторая производная положительная, эта критическая точка является минимумом.\n",
        "\n",
        "Минимум функции $f(x)=x\\cdot\\ln(x)$ достигается при $x = \\frac{1}{e}$"
      ],
      "metadata": {
        "id": "PE5Hi6DdOwph"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZvcqBhMOyWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Задача 2 (2 балла). Бывший президент гугла Сергей подзабыл матанализ. Исследуйте за него на экстремум функцию $f(x,y)=y^2+2xy-4x-2y-3$\n",
        "\n",
        "Частная производная по x:\n",
        "$\\frac{\\partial f}{\\partial x}=2y-4$\n",
        "\n",
        "Частная производная по y:\n",
        "$\\frac{\\partial f}{\\partial y}=2y+2x-2$\n",
        "\n",
        "Установим каждую из этих производных равными нулю, чтобы найти критические точки:\n",
        "\n",
        "$2y-4=0$\n",
        "\n",
        "$y=2$\n",
        "\n",
        "$2y+2x-2=0$\n",
        "\n",
        "$2\\cdot2+2x-2=0=2x+2$\n",
        "\n",
        "$2x=-2$\n",
        "\n",
        "$x=-1$\n",
        "\n",
        "Критическая точка функции $f(x,y)=(-1, 2)$. Чтобы определить, является ли эта точка максимумом, минимумом или седловой точкой, используем критерий Гессе.\n",
        "\n",
        "$f_{xx}=0$\n",
        "\n",
        "$f_{yy}=2$\n",
        "\n",
        "$f_{xy}=2$\n",
        "\n",
        "$H=\\begin{pmatrix}f_{xx}&f_{xy}\\\\f_{xy}&f_{yy}\\end{pmatrix}=\\begin{pmatrix}0&2\\\\2&2\\end{pmatrix}$\n",
        "\n",
        "$det(H)=f_{xx}f_{yy}-(f_{xy})^2=-4$\n",
        "\n",
        "Так как $det(H)<0\\implies(-1, 2)$ является седловой точкой."
      ],
      "metadata": {
        "id": "wPcWOqF0VYfl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fUjXNSO7cPZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 3 (2 балла). Бывший директор Яндекса Аркадий выписал на листочек функцию\n",
        "$f(x,y)=(1+2x+3y)^{2023}$ и просит вас найти её разложение в ряд Тэйлора до второго члена в окрестности точки $x=0,y=0$. Сделайте это.\n",
        "\n",
        "Постоянный член:\n",
        "\n",
        "$f(0,0)=(1+2\\cdot0+3\\cdot0)^{2023}=1^{2023}=1$\n",
        "\n",
        "Линейный член:\n",
        "\n",
        "1. Частная производная по $x$:\n",
        "\n",
        "$f_x=\\frac{\\partial}{\\partial x}\\left((1+2x+3y)^{2023}\\right)=2023\\cdot(1+2x+3y)^{2022}\\cdot2$\n",
        "\n",
        "$f_x(0,0)=2023\\cdot 1^{2022}\\cdot2=4046$\n",
        "\n",
        "2. Частная производная по $y$:\n",
        "\n",
        "$f_y=\\frac{\\partial}{\\partial y}\\left((1+2x+3y)^{2023}\\right)=2023\\cdot(1+2x+3y)^{2022}\\cdot3$\n",
        "\n",
        "$f_y(0,0)=2023\\cdot1^{2022}\\cdot3=6069$\n",
        "\n",
        "Разложение в ряд Тейлора:\n",
        "\n",
        "$f(x,y)\\approx1+4046x+6069y$"
      ],
      "metadata": {
        "id": "tnRcu0M4WcEW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5uHTLW-Zfze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 4 (2 балла). Бывший директор отдела по искусственному интеллекту Теслы Андрей ищет минимум функции $f(x)=ax^2+bx+c$, где $a>0$, методом градиентного спуска. Андрей стартует из точки $x_0$ и настолько ленив, что не хочет делать больше одного шага. При каком значении длины шага $\\gamma$ Андрей за один шаг окажется точно в точке минимума?\n",
        "\n",
        "Для квадратичной функции градиент равен:\n",
        "\n",
        "$\\nabla f(x)=2ax+b$\n",
        "\n",
        "Начиная с точки $x_0$ и делая шаг $\\gamma$, формула градиентного спуска будет выглядеть\n",
        "\n",
        "$x_1=x_0-\\gamma\\cdot\\nabla f(x_0)$\n",
        "\n",
        "Минимум квадратичной функции $f(x)=ax^2+bx+c$, находится в точке $x_{min}=-\\frac{b}{2a}$\n",
        "\n",
        "Для расчёта длины минимального шага примем\n",
        "\n",
        "$x_1=x_{min}\\implies-\\frac{b}{2a}=x_0-\\gamma(2ax_0+b)$\n",
        "\n",
        "$-\\frac{b}{2a}=x_0-2\\gamma ax_0-\\gamma b=x_0(1-2\\gamma a)-\\gamma b$\n",
        "\n",
        "$-\\frac{b}{2a}-x_0=-2\\gamma ax_0-\\gamma b=\\gamma(-2ax_0-b)$\n",
        "\n",
        "$\\gamma=\\frac{-\\frac{b}{2a}-x_0}{-2ax_0-b}=\\frac{\\frac{b}{2a}+x_0}{2ax_0+b}=\\frac{\\frac{b+2ax_0}{2a}}{2ax_0+b}=\\frac{b+2ax_0}{2a}\\cdot\\frac{1}{2ax_0+b}=\\frac{1}{2a}$\n",
        "\n",
        "Андрей достигнет точки минимума за один шаг, если длина шага $\\gamma$ будет равна $\\frac{1}{2a}$."
      ],
      "metadata": {
        "id": "28kT4mgLZhTe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lbENZjZnPTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 5 (2 балла). Бывший директор Microsoft Research Asia, Ли, не доверяет готовым реализациям алгоритмов машинного обучения. Поэтому он написал свой собственный градиентный спуск. Для шага градиентного спуска он использовал следующие формулы: $w_t=w_{t-1}+(\\nabla Q(w_t))^2$\n",
        "\n",
        "1. **В формуле указан положительный знак перед градиентом**:\n",
        "\n",
        "Положительный знак перед градиентом приведет к увеличению значения функции потерь, что может привести к дивергенции вместо сходимости к минимуму.\n",
        "Для минимизации функции потерь следует использовать отрицательный знак перед градиентом:\n",
        "\n",
        "$w_t=w_{t-1}-(\\nabla Q(w_t))^2$\n",
        "\n",
        "2. **В формуле отстутствует коэффицент $\\eta$ — learning rate**.\n",
        "\n",
        "Без коэффицента $\\eta$ — learning rate отсутствует возможность регулировать скорость обучения, и возможно, что выбранная скорость будет слишком неэффективна.\n",
        "\n",
        "-Если градиент имеет слишком большой шаг, веса будут обновляться слишком быстро, что приведет к тому, что алгоритм будет перепрыгивать через минимумы и не сможет сходиться. Это приведет к дивергенции, и модель будет невозможно обучить.\n",
        "\n",
        "-В случае слишком маленького шага алгоритм будет очень долго сходиться к минимуму, что значительно замедлит обучение и может сделать его малоэффективным или неэффективным.\n",
        "\n",
        "Следует добавить learning ratte:\n",
        "\n",
        "$w_t=w_{t-1}-\\eta(\\nabla Q(w_t))^2$\n",
        "\n",
        "3. **В формуле взят квадрат градиента**:\n",
        "\n",
        "Квадрат градиента приведет к очень большим или слишком маленьким значениям шага, что усложнит сходимость и может сделать алгоритм нестабильным. Для улучшения алгоритма следует использовать градиент как таковой (без возведения в степень):\n",
        "\n",
        "$w_t=w_{t-1}-\\eta\\nabla Q(w_t)$\n",
        "\n",
        "4. **В формуле градиент вычисляется в текущей точке $w_t$**\n",
        "\n",
        "При вычислении градиента в текущей точке $w_t$, он еще не известен, так как $w_t$ — это значение, которое мы вычисляем по данной формуле. Использование в формуле $w_t$ создает рекурсивную зависимость, и вычисление становится цикличным приводя к невозможности вычислить градиент. Для правильной реализации алгоритма следует использовать предыдущею точку $w_{t-1}$:\n",
        "\n",
        "$w_t=w_{t-1}-\\eta\\nabla Q(w_{t-1})$\n",
        "\n",
        "Верная формула:\n",
        "\n",
        "$w_t=w_{t-1}-\\eta\\nabla Q(w_{t-1})$\n"
      ],
      "metadata": {
        "id": "Yl7MmFlKnPg0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgnUwwcuv4pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 6 (1 балл). Запишите фамилии Миры, Сергея, Аркадия, Андрея и Ли.\n",
        "\n",
        " 1. Мурати (Мира Мурати)\n",
        " 2. Брин (Сергей Брин)\n",
        " 3. Волож (Аркадий Волож)\n",
        " 4. Карпати (Андрей Карпати)\n",
        " 5. Ли (Ли Кайфу) (англ. Kai-Fu Lee)"
      ],
      "metadata": {
        "id": "14neNDSSv4Zx"
      }
    }
  ]
}